title: 「学习笔记」机器学习(1)
date: 2016-01-22 14:32:57
categories: "机器学习"
tags:
  - 机器学习
  - Python
---
# 科学算法库的安装(linux)

+ 1.安装Numpy
        sudo apt-get install python-numpy
+ 2.安装Scipy
        sudo apt-get install python-numpy
+ 3.Matplotlib
        sudo apt-get install tk-dev
        sudo apt-get install python-gtk2-dev
        sudo apt-get install python-pyside
+ 4.spyder GUI环境
        sudo apt-get install spyder

<br>

+ **上述安装完毕后，可以利用**

    ```python
    #! /usr/bin/python
    import numpy as np
    import matplotlib.pyplot as plt

    x = np.linspace(0,4*3.1415,100)
    y = np.sin(x)

    plt.figure(figsize=(8,4))
    plt.plot(x,y,label="$sin(x)$",color="red",linewidth=2)
    plt.legend()
    plt.show()
    ```

  **进行测试。若生成正弦曲线窗口，则配置完成. **
  
# NumPy的基本操作

+ ## Numpy 的导入

		import numpy as np
    
    这种写法在使用相关函数的时候需要写明是哪个包的，如:

		myZero = np.zeros([3,5])
    
    还可以导入包全局使用

		from numpy import *
    

+ ## NumPy 的基本操作

     1. 创建全0矩阵和全1矩阵
     ```python
      myZero = zeros([n,m])
      myOne = ones([n,m])```

     2. 生成随机矩阵
     ```python
     myRand = random.rand(n,m)   # n 行 m 列的 0～1 之间的随机数矩阵
     ```

     3. 生成单位矩阵
     ```python
     myEye = eye(n)   # n * n 的单位阵
     ```

     4. 将一个数组转化为一个矩阵
     ```python
     myMatrix = mat([[1,2,3],[4,5,6],[7,8,9]])
     ```

     5. 矩阵所有元素求和
     ```python
     S = sum(myMatrix)
     ```

     6. 矩阵各元素的乘积
     ```python
     matrix = multiply(matrix1, matrix2) # matrix1 和 matrix2 对应元素相乘的矩阵
     ```

     7. 求矩阵的 n 次幂
     ```python
     matrix = power(myMatrix, n) #生成一个矩阵，矩阵内部的元素是原矩阵对应元素的n次幂
     ```

     8. 矩阵的转置
     ```python
     print matrix.T   #打印转置后的矩阵，不改变原矩阵
     matrix.transpose()   #同上
     ```

     9. 矩阵的其他操作
     ```python
     [m, n] = shape(matrix) # m, n为矩阵的行列数
     myscl1 = matrix[0] # 矩阵的切片操作，取第一行
     myscl2 = matrix.T[0] # 矩阵的切片操作，取第一列
     mycpmat = matrix.copy() # 矩阵的复制
     print matrix1 < matrix2 # 矩阵的比较，会逐一比较对应的每一个元素，并输出一个仅包含True, False 的相同大小的矩阵
     dot(m1,m2)  #计算m1,m2的点积
     norm(v)  #计算向量V的范数
     ```
+ ## Linalg线性代数库

     1. 矩阵的行列式
     ```python
     print linalg.det(matrix)
     ```
     2. 矩阵的逆
     ```python
     print linalg.inv(matrix)
     ```
     3. 矩阵的对称
     ```python
     print matrix * matrix.T
     ```
     4. 矩阵的秩
     ```python
     print linalg.matrix_rank(A)
     ```
     5. 可逆矩阵求解
     ```python
     print linalg.solve(A,b.T) # 如果b已经是一列的就不用转置了
     ```

+ ## 各类距离的python实现
	 **各类距离会在后面说明**
	 1. Euclidean Distance
	 ```python
     vector1 = mat([1,2,3])
     vector2 = mat([4,5,6])
     print sqrt((vector1-vector2)*(vector1-vector2).T)
     ```
	 2. Manhattan Distance
	 ```python
     vector1 = mat([1,2,3])
     vector2 = mat([4,5,6])
     print sum(abs(vector1-vector2))
     ```
	 3. Chebyshev Distance
	 ```python
     vector1 = mat([1,2,3])
     vector2 = mat([4,5,6])
     print abs(vector1-vector2).max()
     ```
	 4. Cosine
	 ```python
     cosV12 = dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))
     print cosV12
     ```
	 5. Hamming Distance
	 ```python
     matV = mat([[1,1,0,1,0,1,0,0,1], [0,1,1,0,0,0,1,1,1]])
     smstr = nonzero(matV[0] - matV[1]);
     print shape(smstr[0])[1]
     ```
	 6. Jaccard Distance
	 ```python
     import scipy.spatial.distance as dist
     matV = mat([[1,1,0,1,0,1,0,0,1], [0,1,1,0,0,0,1,1,1]])
     print dist.pdist(matV, 'jaccard')
     ```

     
# 机器学习的数学基础

 + **范数**
 > 向量的范数可以简单、形象的理解为**向量的长度**，或者向量到坐标系原点的距离，或者**相应空间内的两点之间的距离**
 >  
 > ***向量的范数定义*** : 向量的范数是一个函数 $ \parallel x\parallel $ ,满足非负性 $ \parallel x\parallel > 0 $ , 齐次性 $ \parallel cx\parallel = \mid c\mid\parallel x\parallel $ ,三角不等式 $ \parallel x+y\parallel \leq\parallel x\parallel +\parallel y\parallel $ 。
 > 
 > L1范数：  $\parallel x\parallel $为 $ x $向量各个元素绝对值之和。
 > L2范数：  $\parallel x\parallel $为 $ x $向量各个元素平方和的开方，又称 Euclidean 范数或者 Frobenius 范数。
 > Lp范数：  $\parallel x\parallel $为 $ x $向量各个元素绝对值 $ p $次方和的  $ 1\over p $ 次方
 > L $\infty $范数：  $\parallel x\parallel $为 $ x $向量各个元素绝对值最大的那个元素，如下：$$ \lim\_{k\to\infty}\left( \sum\_{i=1}^n\mid p\_i-q\_i\mid ^k\right)^\frac{1}{k}
 $$
 + **Minkowski Distance (闵可夫斯基距离)**
 > 严格意义上讲，Minkowski Distance 不是一种距离，而是一组距离的定义。
 > 两个n维向量 $ A(x\_{11},x\_{12},\cdots,x\_{1n}) $ 与 $ B(x\_{21},x\_{22},\cdots,x\_{2n}) $ 间的Minkowski距离定义为：
 > $$ d\_{12}=\sqrt[p]{\sum\_{k=1}^n(x\_{1k}-x\_{2k})^p} $$
 > 其中p是一个变参数。
 > - 当 p=1 时，就是 Manhattan Distance (曼哈顿距离)
 > - 当 p=2 时，就是 Euclidean Distance (欧氏距离)
 > - 当 $ p\to\infty $ 时，就是 Chebyshev Distance (切比雪夫距离)
 + **Euclideam Distance**
 > 欧氏距离（L2范数）是最易于理解的一种距离计算方法，源于欧氏空间的两点距离公式
 > 两个n维向量 $ A(x\_{11},x\_{12},\cdots,x\_{1n}) $ 与 $ B(x\_{21},x\_{22},\cdots,x\_{2n}) $ 之间的欧氏距离：
 > $$ d\_{12}=\sqrt{\sum\_{k=1}^n(x\_{1k}-x\_{2k})^2} $$
 > 表示为向量运算的形式：
 > $$ d\_{12}=\sqrt{(A - B)(A - B)^T} $$
 + **Manhattan Distance**
 > 曼哈顿距离（L1范数）可以理解为计算网格中两点路径的距离
 > 二维平面两点 $ A(x\_1,y\_1) $ 和 $ B(x\_2,y\_2) $ 间的曼哈顿距离:
 > $$ d\_{12}=\mid x\_1-x\_2\mid +\mid y\_1-y\_2\mid $$
 > 两个n维向量 $ A(x\_{11},x\_{12},\cdots,x\_{1n}) $ 与 $ B(x\_{21},x\_{22},\cdots,x\_{2n}) $ 之间的曼哈顿距离：
 > $$ d\_{12}=\sum\_{k=1}^n\mid x\_{1k}-x\_{2k}\mid $$
 + **Chebyshev Distance**
 > 切比雪夫距离类似与棋盘上国王从一点到另一点移动的最少次数，即 $ max(\mid x\_1-x\_2\mid,\mid y\_1-y\_2\mid) $
 > 两个n维向量 $ A(x\_{11},x\_{12},\cdots,x\_{1n}) $ 与 $ B(x\_{21},x\_{22},\cdots,x\_{2n}) $ 之间的切比雪夫距离：
 > $$ d\_{12}=max\_i(\mid x\_{1i}-x\_{2i}\mid) $$
 > 该公式的另一个等价公式：
 > $$ d\_{12}=\lim\_{k\to\infty}\left(\sum\_{i=1}^n\mid x\_{1i}-x\_{2i}\mid^k\right)^\frac{1}{k} $$
 + **Cosine**
 > 夹角余弦可以用来两个向量方向的差异，机器学习中借用这一概念来衡量样本之间的差异
 > 两个n维向量 $ A(x\_{11},x\_{12},\cdots,x\_{1n}) $ 与 $ B(x\_{21},x\_{22},\cdots,x\_{2n}) $ 之间的夹角余弦：
 > $$ \cos\theta=\frac{AB}{\mid A\mid\mid B\mid} $$
 > 即：
 > $$ \cos\theta=\frac{\sum\_{k=1}^nx\_{1k}x\_{2k}}{\sqrt{\sum\_{k=1}^nx\_{1k}^2}\sqrt{\sum\_{k=1}^nx\_{2k}^2}} $$
 + **Hamming Distance**
 > 汉明距离的定义：两个等长字符串s1,s2,将其中一个变成另一个需要的最小替换次数。
 > 应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）
 + **Jaccard Similarity Coefficient(杰卡德相似系数)**
 > 杰卡德相似系数：两个集合A,B的交集元素在并集元素中所占的比例，用符号 $ J(A,B) $ 表示
 > $$ J(A,B)=\frac{\mid A\cap B\mid}{\mid A\cup B\mid} $$
 > 杰卡德距：与杰卡德相似系数相反的概念：
 > $$ J\_\delta(A,B)=1-J(A,B)=\frac{\mid A\cup B\mid-\mid A\cap B\mid}{\mid A\cup B\mid} $$
 + **特征间的相关性**
 > 1. 相关系数与相关距离
 > 	相关系数：
 > 	$$ \rho\_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\frac{E((X-EX)(Y-EY))}{\sqrt{D(X)}\sqrt{D(Y)}} $$
 > 	相关距离：
 > 	$$ D\_{XY}=1-\rho\_{XY} $$
  	python实现：
  	```python
  	featuremat = mat(...)  # 初始化矩阵
  	# 计算均值
  	mv1 = mean(featuremat[0])  # 计算第一列的均值
  	mv2 = mean(featuremat[1])  # 计算第二列的均值
  	#计算两列的标准差
  	dv1 = std(featuremat[0]) 
  	dv2 = std(featuremat[1])
  	corref = mean(multiply(featuremat[0]-mv1,featuremat[1]-mv2))/(dv1*dv2)
  	print corref  #输出相关系数
  	print corrcoef(featuremat)  #输出相关系数矩阵
  	```
 > 2. 马氏距离